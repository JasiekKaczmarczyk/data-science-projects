{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "\n",
    "import vgg_neural_style_transfer as nst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (17): ReLU(inplace=True)\n",
      "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (24): ReLU(inplace=True)\n",
      "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (26): ReLU(inplace=True)\n",
      "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (31): ReLU(inplace=True)\n",
      "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (33): ReLU(inplace=True)\n",
      "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (35): ReLU(inplace=True)\n",
      "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.vgg19(pretrained=True).features\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_losses(original_features: torch.Tensor, style_features: torch.Tensor, generated_features: torch.Tensor):\n",
    "    # loss functions\n",
    "        original_loss = 0\n",
    "        style_loss = 0\n",
    "\n",
    "        # iterating through each feature layer\n",
    "        for original_feature, style_feature, generated_feature in zip(original_features, style_features, generated_features):\n",
    "\n",
    "            # shape of images for certain feature layer\n",
    "            _, channel, height, width = generated_feature.shape\n",
    "\n",
    "            # gram matrices for generated and style\n",
    "            generated_gram_matrix = torch.mm(\n",
    "                generated_feature.reshape(channel, height*width),\n",
    "                generated_feature.reshape(channel, height*width).t()\n",
    "                )\n",
    "\n",
    "            style_gram_matrix = torch.mm(\n",
    "                style_feature.reshape(channel, height*width),\n",
    "                style_feature.reshape(channel, height*width).t()\n",
    "                )\n",
    "\n",
    "            # mse\n",
    "            original_loss += torch.mean((generated_feature-original_feature)**2)\n",
    "            style_loss += torch.mean((generated_gram_matrix-style_gram_matrix)**2)\n",
    "\n",
    "        return original_loss, style_loss\n",
    "\n",
    "\n",
    "def train(model: nn.Module, original_image: torch.Tensor, style_image: torch.Tensor, generated_image: torch.Tensor, epochs: int, learning_rate: float, alpha: float, beta: float):\n",
    "    \"\"\"\n",
    "    Training loop\n",
    "    \"\"\"\n",
    "\n",
    "    # setting up optimizer to optimize generated_image\n",
    "    optimizer = optim.Adam([generated_image], lr=learning_rate)\n",
    "\n",
    "    for step in range(epochs):\n",
    "        # grabbing features in each feature layer for each image\n",
    "        original_features = model(original_image)\n",
    "        style_features = model(style_image)\n",
    "        generated_features = model(generated_image)\n",
    "\n",
    "        # calculate losses\n",
    "        original_loss, style_loss = calculate_losses(original_features, style_features, generated_features)\n",
    "\n",
    "        # counting total loss\n",
    "        total_loss = alpha * original_loss + beta * style_loss\n",
    "\n",
    "\n",
    "        # backpropagation and gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # printing loss and saving image\n",
    "        if step % 500 == 0:\n",
    "            print(f\"Step: {step} Loss: {total_loss}\")\n",
    "            save_image(generated_image, f\"images/generated_step{step}.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layers = ['0', '5', '10', '19', '28']\n",
    "\n",
    "model = nst.VGG_NST(feature_layers).to(device)\n",
    "\n",
    "image_size = 512\n",
    "learning_rate = 0.001\n",
    "epochs = 6000\n",
    "alpha = 1\n",
    "beta = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor()\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def load_image(image_name, image_transform):\n",
    "    image = Image.open(image_name)\n",
    "    image = image_transform(image).unsqueeze(0)\n",
    "\n",
    "    return image.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = load_image(\"images/original.jpg\", image_transforms)\n",
    "style_image = load_image(\"images/style_image.jpg\", image_transforms)\n",
    "\n",
    "generated_image = original_image.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, original_image, style_image, generated_image, epochs, learning_rate, alpha, beta)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96959798ab2ac4a3a0205a99ed6dcf819e735c1fab48404ed0b1d398803c3294"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
